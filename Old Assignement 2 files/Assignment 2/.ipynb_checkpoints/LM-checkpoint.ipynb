{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from PorterStemmer import *\n",
    "#from nltk.stem import PorterStemmer\n",
    "#from nltk.tokenize import word_tokenize\n",
    "ps = PorterStemmer()\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "delim = '''[ ',(){}.:;\"`\\n]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"C:\\Files\\col764-a2-release\\col764-ass2-release\\data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = os.path.join(data_dir,\"metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kshitij alwadhi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3063: DtypeWarning: Columns (1,4,5,6,13,14,15,16) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "meta_df = pd.read_csv(metadata)\n",
    "meta_df = meta_df[['cord_uid','title','abstract','pdf_json_files','pmc_json_files']]\n",
    "#meta_df = meta_df.head(1000)\n",
    "meta_df['pdf_json_files'] = meta_df['pdf_json_files'].astype(str)\n",
    "meta_df['pmc_json_files'] = meta_df['pmc_json_files'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTokensFromText(text):\n",
    "    words = re.split(delim, text)\n",
    "    res = []\n",
    "    for w in words:\n",
    "        if(len(w) > 2):\n",
    "            #temp = ps.stem(w.lower(), 0, len(w)-1)\n",
    "            temp = w.lower()\n",
    "            if temp not in stop_words and not re.search('[0-9]+',temp):\n",
    "                res.append(temp)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_file = \"covid19-topics.xml\"\n",
    "top100_file= \"t40-top-100.txt\"\n",
    "relevance_file = \"t40-qrels.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_queries():\n",
    "    with open(os.path.join(data_dir,topics_file), \"r\") as file:\n",
    "        content = file.readlines()\n",
    "        content = \"\".join(content)\n",
    "        bs_content = BeautifulSoup(content, \"lxml\")\n",
    "        queries = bs_content.find_all(\"query\")\n",
    "        all_text = []\n",
    "        for q in queries:\n",
    "            all_text.append(q.text)\n",
    "        return all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevances():\n",
    "    rel_dict = {}\n",
    "    with open(os.path.join(data_dir,relevance_file),\"r\") as file:\n",
    "        for x in file:\n",
    "            line = x.split(\" \")\n",
    "            qid = int(line[0])\n",
    "            docid = line[2]\n",
    "            rel = int(line[3].replace(\"\\n\",\"\"))\n",
    "            if qid not in rel_dict:\n",
    "                rel_dict[qid] = {docid:rel}\n",
    "            else:\n",
    "                rel_dict[qid][docid] = rel\n",
    "    return rel_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top100():\n",
    "    top100_dict = {}\n",
    "    with open(os.path.join(data_dir,top100_file),\"r\") as file:\n",
    "        for x in file:\n",
    "            line = x.split(\" \")\n",
    "            if(len(line)>1):\n",
    "                qid = int(line[0])\n",
    "                docid = line[2]\n",
    "                if qid not in top100_dict:\n",
    "                    top100_dict[qid] = [docid]\n",
    "                else:\n",
    "                    top100_dict[qid].append(docid)\n",
    "    return top100_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "top100_dict = get_top100()\n",
    "rel_dict = get_relevances()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_dict(docid):\n",
    "    tf_dict = {}\n",
    "    paths = filemap[docid]\n",
    "    alltext = \"\"\n",
    "    for path in paths.split(\"; \"):\n",
    "        if(path == \"nan\"):\n",
    "            continue\n",
    "        datapath = os.path.join(data_dir,path)\n",
    "        with open(datapath) as data:\n",
    "            data_dict = json.load(data)\n",
    "            for section in data_dict['body_text']:\n",
    "                section_text = section['text']\n",
    "                alltext+=section_text\n",
    "                alltext+=\" \"\n",
    "    tokens = getTokensFromText(alltext)\n",
    "    for token in tokens:\n",
    "        if token not in tf_dict:\n",
    "            tf_dict[token]=1\n",
    "        else:\n",
    "            tf_dict[token]+=1\n",
    "    return tf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"filemap.json\", \"r\") as jsonfile:\n",
    "    filemap = json.load(jsonfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFreqMap(qno):\n",
    "    # tf map from word->doc-> freq\n",
    "    top100docs = top100_dict[qno]\n",
    "    freq_map = {}\n",
    "    doclen_map ={}\n",
    "    for doc in (top100docs):\n",
    "        tf_dict = get_tf_dict(doc)\n",
    "        total = 0\n",
    "        for entry in tf_dict.items():\n",
    "            token = entry[0]\n",
    "            freq = entry[1]\n",
    "            total +=freq\n",
    "            if token not in freq_map:\n",
    "                freq_map[token] = {}\n",
    "            freq_map[token][doc] = freq\n",
    "        doclen_map[doc] = total\n",
    "    vocab = list(freq_map.keys())\n",
    "    \n",
    "    return freq_map,doclen_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26343"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_map,doclen_map = getFreqMap(2)\n",
    "len(list(freq_map.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getProb_dict(qno,mu):\n",
    "    freq_map,doclen_map = getFreqMap(qno)\n",
    "    vocab = list(freq_map.keys())\n",
    "    # map word->doc->prob\n",
    "    prob_map = {}\n",
    "    tf_dict = {}\n",
    "    for w in vocab:\n",
    "        tot = 0\n",
    "        for doc in freq_map[w].keys():\n",
    "            t1 = freq_map[w][doc]\n",
    "            tot+=t1\n",
    "        tf_dict[w] = tot\n",
    "    for w in vocab:\n",
    "        for doc in freq_map[w].keys():\n",
    "            t1 = freq_map[w][doc]\n",
    "            t2 = tf_dict[w]\n",
    "            prob = (t1+mu*t2)/(doclen_map[doc] + mu)\n",
    "            if w not in prob_map:\n",
    "                prob_map[w] = {}\n",
    "            prob_map[w][doc] = prob\n",
    "    return prob_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jm18lj5t': 0.0012555342628692262, '8ywd6j2b': 0.0011781536648994362}"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getProb_dict(1,0.6)['weather']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_map = {}\n",
    "for i,q_orig in (enumerate(get_queries())):\n",
    "    q = q_orig.lower()\n",
    "    q_map[i+1] = q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopkExp(qno,k,mu):\n",
    "    prob_map = getProb_dict(qno,mu)\n",
    "    q = q_map[qno]\n",
    "    top100docs = top100_dict[qno]\n",
    "    qtokens = getTokensFromText(q)\n",
    "    # map from word -> finProb\n",
    "    finmap = {}\n",
    "    vocab = list(prob_map.keys())\n",
    "    for w in vocab:\n",
    "        total = 0\n",
    "        for doc in top100docs:\n",
    "            prob = 1\n",
    "            for qi in qtokens:\n",
    "                if doc in prob_map[qi]:\n",
    "                    prob*=prob_map[qi][doc]\n",
    "                else:\n",
    "                    prob = 0\n",
    "            if doc in prob_map[w]:\n",
    "                prob*=prob_map[w][doc]\n",
    "            else:\n",
    "                prob = 0\n",
    "            total += prob\n",
    "        finmap[w] = total\n",
    "    sorted_tokens = dict(sorted(finmap.items(), key=lambda x: x[1],reverse = True))\n",
    "    sorted_tokens = list(sorted_tokens.keys())\n",
    "    return sorted_tokens[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopkExpR2(qno,k,mu):\n",
    "    prob_map = getProb_dict(qno,mu)\n",
    "    q = q_map[qno]\n",
    "    top100docs = top100_dict[qno]\n",
    "    qtokens = getTokensFromText(q)\n",
    "    # map from word -> finProb\n",
    "    finmap = {}\n",
    "    vocab = list(prob_map.keys())\n",
    "    for w in vocab:\n",
    "        total = 0\n",
    "        for doc in top100docs:\n",
    "            if doc in prob_map[w]:\n",
    "                total+=prob_map[w][doc]\n",
    "        p_w = total\n",
    "        prob = p_w * p_w\n",
    "        \n",
    "        for qi in qtokens:\n",
    "            total = 0\n",
    "            for doc in top100docs:\n",
    "                temp = 1\n",
    "                if doc in prob_map[w]:\n",
    "                    temp*= prob_map[w][doc]\n",
    "                else:\n",
    "                    temp*=0\n",
    "                if doc in prob_map[qi]:\n",
    "                    temp*=prob_map[qi][doc]\n",
    "                else:\n",
    "                    temp*=0\n",
    "                total+=temp\n",
    "            prob*=total\n",
    "        finmap[w] = total\n",
    "    sorted_tokens = dict(sorted(finmap.items(), key=lambda x: x[1],reverse = True))\n",
    "    sorted_tokens = list(sorted_tokens.keys())\n",
    "    return sorted_tokens[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7157f73fc3084cecbe7698ecdbb14aa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['patients',\n",
       " 'remdesivir',\n",
       " 'treatment',\n",
       " 'clinical',\n",
       " 'virus',\n",
       " 'viral',\n",
       " 'rna',\n",
       " 'antiviral',\n",
       " 'coronavirus',\n",
       " 'drug',\n",
       " 'respiratory',\n",
       " 'drugs',\n",
       " 'disease',\n",
       " 'severe',\n",
       " 'infection',\n",
       " 'used',\n",
       " 'also',\n",
       " 'use',\n",
       " 'activity',\n",
       " 'results']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getTopkExp(30,20,0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['virus',\n",
       " 'temperature',\n",
       " 'may',\n",
       " 'weather',\n",
       " 'cases',\n",
       " 'also',\n",
       " 'data',\n",
       " 'health',\n",
       " 'disease',\n",
       " 'study',\n",
       " 'number',\n",
       " 'time',\n",
       " 'climate',\n",
       " 'conditions',\n",
       " 'new',\n",
       " 'transmission',\n",
       " 'change',\n",
       " 'population',\n",
       " 'respiratory',\n",
       " 'used']"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getTopkExpR2(2,20,0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'coronavirus origin',\n",
       " 2: 'coronavirus response to weather changes',\n",
       " 3: 'coronavirus immunity',\n",
       " 4: 'how do people die from the coronavirus',\n",
       " 5: 'animal models of covid-19',\n",
       " 6: 'coronavirus test rapid testing',\n",
       " 7: 'serological tests for coronavirus',\n",
       " 8: 'coronavirus under reporting',\n",
       " 9: 'coronavirus in canada',\n",
       " 10: 'coronavirus social distancing impact',\n",
       " 11: 'coronavirus hospital rationing',\n",
       " 12: 'coronavirus quarantine',\n",
       " 13: 'how does coronavirus spread',\n",
       " 14: 'coronavirus super spreaders',\n",
       " 15: 'coronavirus outside body',\n",
       " 16: 'how long does coronavirus survive on surfaces',\n",
       " 17: 'coronavirus clinical trials',\n",
       " 18: 'masks prevent coronavirus',\n",
       " 19: 'what alcohol sanitizer kills coronavirus',\n",
       " 20: 'coronavirus and ace inhibitors',\n",
       " 21: 'coronavirus mortality',\n",
       " 22: 'coronavirus heart impacts',\n",
       " 23: 'coronavirus hypertension',\n",
       " 24: 'coronavirus diabetes',\n",
       " 25: 'coronavirus biomarkers',\n",
       " 26: 'coronavirus early symptoms',\n",
       " 27: 'coronavirus asymptomatic',\n",
       " 28: 'coronavirus hydroxychloroquine',\n",
       " 29: 'coronavirus drug repurposing',\n",
       " 30: 'coronavirus remdesivir',\n",
       " 31: 'difference between coronavirus and flu',\n",
       " 32: 'coronavirus subtypes',\n",
       " 33: 'coronavirus vaccine candidates',\n",
       " 34: 'coronavirus recovery',\n",
       " 35: 'coronavirus public datasets',\n",
       " 36: 'sars-cov-2 spike structure',\n",
       " 37: 'sars-cov-2 phylogenetic analysis',\n",
       " 38: 'covid inflammatory response',\n",
       " 39: 'covid-19 cytokine storm',\n",
       " 40: 'coronavirus mutations'}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getQueryVector(query,qno,k,mu,vocab):\n",
    "    tokens = getTokensFromText(query)\n",
    "    tokens.extend(getTopkExp(qno,k,mu))\n",
    "    query_tf_dict = {}\n",
    "    for token in tokens:\n",
    "        if token not in query_tf_dict:\n",
    "            query_tf_dict[token]=1\n",
    "        else:\n",
    "            query_tf_dict[token]+=1\n",
    "    vec = np.zeros(len(vocab))\n",
    "    freq_map,doclen_map = getFreqMap(qno)\n",
    "    for word,tf in query_tf_dict.items():\n",
    "        if word in vocab.keys():\n",
    "            pos = vocab[word]\n",
    "            normtf = 1 + math.log(tf,2)\n",
    "            #normidf = get_idf(word)\n",
    "            idf = len(freq_map[word])\n",
    "            normidf = math.log(1+100/idf,2)\n",
    "            tfidf = normtf * normidf\n",
    "            vec[pos] = tfidf\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "736e4c30bbaa4ca6a424565ea775ae90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_dict = {}\n",
    "for i,q_orig in tqdm(enumerate(get_queries())):\n",
    "    qno = i+1\n",
    "    freq_map,doclen_map = getFreqMap(qno)\n",
    "    vocab = {}\n",
    "    for i,word in enumerate(freq_map.keys()):\n",
    "        vocab[word] = i\n",
    "    vocab_dict[qno] = vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LM_updateQueries(k,mu):\n",
    "    updated_queries = []\n",
    "    for i,q_orig in tqdm(enumerate(get_queries())):\n",
    "        q = q_orig.lower()\n",
    "        qno = i+1\n",
    "        vocab = vocab_dict[qno]\n",
    "        qv0 = getQueryVector(q,qno,k,mu,vocab)\n",
    "        updated_queries.append(qv0)\n",
    "    return updated_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b5b19980617469fbcbfd85adcd51fb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "updated_queries = LM_updateQueries(20,0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDocVector(docid,vocab,freq_map):\n",
    "    tf_dict = get_tf_dict(docid)\n",
    "    vec = np.zeros(len(vocab))\n",
    "    for word,tf in tf_dict.items():\n",
    "        if word in vocab:\n",
    "            pos = vocab[word]\n",
    "            normtf = 1 + math.log(tf,2)\n",
    "            idf = len(freq_map[word])\n",
    "            normidf = math.log(1+100/idf,2)\n",
    "            tfidf = normtf * normidf\n",
    "            vec[pos] = tfidf\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNDCG(qno,docs):\n",
    "    def discount_helper(arr):\n",
    "        score = 0\n",
    "        for i,e in enumerate(arr):\n",
    "            score += e/math.log(i+2,2)\n",
    "        return score\n",
    "    scores = []\n",
    "    for doc in docs:\n",
    "        if doc in rel_dict[qno]:\n",
    "            scores.append(rel_dict[qno][doc])\n",
    "        else:\n",
    "            scores.append(0)\n",
    "    origscore = discount_helper(scores)\n",
    "    sortedscore = discount_helper(sorted(scores,reverse=True))\n",
    "    if sortedscore == 0:\n",
    "        return 0\n",
    "    return origscore/sortedscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSim(queryVec,docVec):\n",
    "    if np.linalg.norm(queryVec) == 0 or np.linalg.norm(docVec)==0:\n",
    "        return 0.0\n",
    "    return np.dot(queryVec,docVec)/(np.linalg.norm(queryVec) * np.linalg.norm(docVec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LM_getNDCG(updated_queries):\n",
    "    total = 0\n",
    "    allscores = []\n",
    "    for i,q in tqdm(enumerate(updated_queries)):\n",
    "        qno = i+1\n",
    "        freq_map,doclen_map = getFreqMap(qno)\n",
    "        doc_score_dict = {}\n",
    "        top100docs = top100_dict[qno]\n",
    "        vocab = vocab_dict[qno]\n",
    "        for doc in top100docs:\n",
    "            score = getSim(q,getDocVector(doc,vocab,freq_map))\n",
    "            doc_score_dict[doc] = score\n",
    "        sorted_docs = dict(sorted(doc_score_dict.items(), key=lambda x: x[1],reverse = True))\n",
    "        sorted_docs = list(sorted_docs.keys())\n",
    "        ndcg = getNDCG(qno,sorted_docs)\n",
    "        allscores.append(ndcg)\n",
    "        total +=ndcg\n",
    "    return allscores,total/len(updated_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e8cfa66cbd34dd289bd957fbc591c5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.8364713928251899,\n",
       "  0.8490650495555893,\n",
       "  0.7459413854057952,\n",
       "  0.25711046737534016,\n",
       "  0.6642047612890934,\n",
       "  0.9241256794184907,\n",
       "  0.9305632475767501,\n",
       "  0.38097179678159365,\n",
       "  0,\n",
       "  0.8151962527342926,\n",
       "  0.5099261368517586,\n",
       "  0.7199773775799176,\n",
       "  0.4583099850885371,\n",
       "  0.7780650091691769,\n",
       "  0.5138749241456431,\n",
       "  0.7845182542274431,\n",
       "  0.8599859952763825,\n",
       "  0.9282851645109501,\n",
       "  0.6549040069641665,\n",
       "  0.9655616430301298,\n",
       "  0.8107292573564864,\n",
       "  0.8928692285678204,\n",
       "  0.8834844738000237,\n",
       "  0.9273835820616646,\n",
       "  0.8205136415143893,\n",
       "  0.8503490718309931,\n",
       "  0.932127583573335,\n",
       "  0.9428005952239759,\n",
       "  0.8516524622049824,\n",
       "  0.8831539646074478,\n",
       "  0.3520754855748004,\n",
       "  0.2839494431967705,\n",
       "  0.7196372486008927,\n",
       "  0.6190450953833722,\n",
       "  0.6640565901693796,\n",
       "  0.9767514386170004,\n",
       "  0.7783355860841272,\n",
       "  0.9453382783356529,\n",
       "  0.9664697328709919,\n",
       "  0.8378654121803345],\n",
       " 0.7378911675390171)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LM_getNDCG(updated_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [['a','b'],['c','d']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : a,b\n",
      "2 : c,d\n"
     ]
    }
   ],
   "source": [
    "for i,ex in enumerate(a):\n",
    "    ll = \"\"\n",
    "    for e in ex:\n",
    "        ll+=e\n",
    "        ll+=\",\"\n",
    "    l = str(i+1) + \" : \"+ ll[:-1]\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
